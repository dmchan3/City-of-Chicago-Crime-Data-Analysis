---
title: "City of Chicago Crime Data Analysis Project Proposal"
date: "11-03-24"
output: html_document
---

## Research Topic
This project will analyze City of Chicago crime data to explore patterns and insights over the past 5 years. Using this large dataset (200-300k records per year), our aim is to identify trends and examine crime intensity based on locations, times, and types of offenses. Specifically, we will explore questions such as which blocks or streets are the most dangerous, seasonal crime patterns, and how these trends have evolved.

## Team Members
- David Chan
- Croix Westbrock
- Srishti Nandal 

## Data

### Description of the Dataset
Our primary dataset comes from the City of Chicago government open data portal, containing records of reported crimes. The link to the full dataset (2001-Present) is https://data.cityofchicago.org/Public-Safety/Crimes-2001-to-Present/ijzp-q8t2/about_data . For this project, we are using a subset of this dataset to focus on the last 5 years. CSV files for specific years, as we will be using, can be found on the site linked above by searching "Crimes - *Year*". Each record includes:
- **Date and time** of the crime
- **Location description** (block and street)
- **Type of crime** (categorized into different types like theft, assault, robbery, etc.)
- **Arrest** status
- **Domestic** status
- **Ward** and **Community Area**

### Initial Data Cleaning Steps
1. **Importing the Data**: Load data for the last 4-5 years to keep the dataset manageable while preserving recent trends.
```{r}
library(tidyverse)
data2024_1 <- read.csv("2020_1.csv")
data2024_1

```

2. **Date and Time Processing**: Parse the date and time columns to allow for time-based analysis (e.g., crime by hour, day, month, and year).
3. **Filtering by Location**: Extract latitude and longitude or ward/community areas to focus on specific regions or hotspots in Chicago.
4. **Handling Missing Data**: Identify and address missing values, particularly in columns critical to our analysis (e.g., location or crime type).
5. **Subsetting Data for Efficiency**: If initial dataset sizes cause computational slowdowns, we may start with smaller subsets and scale up to the full dataset as we optimize our code.

### Code for Data Cleaning
```{r setup, include=FALSE}
# Load necessary libraries
library()

# Load the data for recent years 

# Initial data processing

